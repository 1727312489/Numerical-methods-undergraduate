{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Numerical Differentiation\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* Learn about finite difference approximations to derivatives.\n",
    "* Be able to implement forward and central difference methods.\n",
    "* Calculate higher-order derivatives.\n",
    "* Solve simple ODEs using the finite difference method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite differences -- the forward difference\n",
    "\n",
    "Finite differences are a class of approximation methods for estimating/computing derivatives of functions.\n",
    "\n",
    "Approximations to the derivatives of a function can be computed by using weighted sums of function evaluations at a number of points. The elementary definition of the derivative of a function $f$ at a point $x_0$ is given by:\n",
    "\n",
    " $$ f'(x_0)=\\lim_{h\\rightarrow 0} \\frac{f(x_0+h)-f(x_0)}{h} $$\n",
    "\n",
    "We can turn this into an approximation rule for $f'(x)$ by replacing the limit as $h$ approaches $0$ with a small but finite $h$:\n",
    "\n",
    " $$ f'(x_0)\\approx \\frac{f(x_0+h)-f(x)}{h},\\qquad h>0 $$\n",
    "\n",
    "The figure below illustrates this approximation. Because the approximate gradient is calculated using values of $x$ greater than $x_0$, this algorithm is known as the **forward difference method**. In the figure the derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line -- if the second (and/or higher) derivative of the function is large then this approximation might not be very good unless you make $h$ very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forward difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/forward_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor series to estimate accuracy\n",
    "We can use a [Taylor series expansion](http://mathworld.wolfram.com/TaylorSeries.html) to estimate the accuracy of the method. Recall that Taylor series in one dimention tells us that we can expand an increment to the evaluation point of a function as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x_0+h)&=f(x_0)+hf'(x_0)+ \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\ldots\\\\ & =f(x_0)+hf'(x_0)+O(h^2)\n",
    "\\end{align*}\n",
    " \n",
    "where $O(h^2)$ represents the collection of terms that are second-order in $h$ or higher.\n",
    "\n",
    "If we rearrange this expression to isolate the gradient term $f'(x_0)$ on the left hand side, we find:\n",
    "\n",
    " $$ hf'(x_0)=f(x_0+h)-f(x_0) +O(h^2) $$\n",
    " \n",
    "and therefore, by dividing through by $h$,\n",
    " \n",
    " $$ f'(x_0)=\\frac{f(x_0+h)-f(x_0)}{h}+O(h) $$\n",
    "\n",
    "As we are left with $O(h)$ at the end, we know that the forward difference method is first-order (i.e. $h^1$) -- as we make the spacing $h$ smaller we expect the error in our derivative to fall linearly.\n",
    "\n",
    "For general numerical methods we generally strive for something better than this -- if we halve our $h$ (and so are doing twice as much (or more) work potentially) we would like our error to drop super-linearly: i.e. by a factor of 4 (second-order method) or 8 (third-order method) or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.1: Compute first derivative using forward differencing</span>\n",
    "\n",
    "Use the forward difference scheme to compute an approximation to $f'(2.36)$ from the following data:\n",
    "\n",
    "$f(2.36) = 0.85866$\n",
    "\n",
    "$f(2.37) = 0.86289$\n",
    "\n",
    "You should get an answer of 0.423."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central differencing\n",
    "\n",
    "In an attempt to derive a more accurate method, we use two Taylor series expansions; one in the positive $x$ direction from $x_0$, and one in the negative direction. Because we hope to achieve better than first order, we include an extra term in the series:\n",
    "\n",
    "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "\n",
    "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{(-h)^2}{2}f''(x_0) + O((-h)^3) $$\n",
    "\n",
    "Using the fact that $(-h)^2=h^2$ and the absolute value signs from the definition of $O$, this is equivalent to:\n",
    "\n",
    "$$ f(x_0+h)=f(x_0)+hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "  \n",
    "$$ f(x_0-h)=f(x_0)-hf'(x_0)+\\frac{h^2}{2}f''(x_0) + O(h^3) $$\n",
    "  \n",
    "Remember that we are looking for an expression for $f'(x_0)$. Noticing the sign change between the derivative terms in the two equations, we subtract the bottom equation from the top equation to give:\n",
    "\n",
    "$$ f(x_0+h)-f(x_0-h)=2hf'(x_0) + O(h^3) $$\n",
    "\n",
    "Finally, rearrange to get an expression for $f'(x_0)$:\n",
    "\n",
    "$$ f'(x_0)=\\frac{f(x_0+h)-f(x_0-h)}{2h} + O(h^2) $$\n",
    "\n",
    "We can see that by taking an interval symmetric about $x_0$, we have created a second-order approximation for the derivative of $f$. This symmetry gives the scheme its name: the central difference method. The figure below illustrates this scheme. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.  \n",
    "\n",
    "Even without the analysis above it's hopefully clear visually why this should in general give a lower error than the forward difference approach. However the analysis of the two methods does tell us that as we halve $h$ the error should drop by a factor 4 rather than the 2 we get for the first-order forward differencing.\n",
    "\n",
    "![\"Central difference method for approximating $f'(x_0)$. The derivative is approximated by the slope of the red line, while the true derivative is the slope of the blue line.\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/central_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.2: Compute first derivative using central differencing</span>\n",
    "\n",
    "Use the data below to compute $f'(0.2)$ using central differencing:\n",
    "\n",
    "$$f(0.1) = 0.078348$$\n",
    "$$f(0.2) = 0.138910$$\n",
    "$$f(0.3) = 0.192916$$\n",
    "\n",
    "You should get 0.57284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Example: Write a function to perform numerical differentiation</span>\n",
    "\n",
    "As covered above, the formula\n",
    "\n",
    "$$f^\\prime(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$$\n",
    "\n",
    "can be used to find an approximate derivative of a mathematical function $f(x)$ if $h$ is small. \n",
    "\n",
    "Write a function *diff*( *f*, *x*, *h*=1E-6) that returns the approximation of the derivative of a mathematical function represented by a Python function f(x).\n",
    "\n",
    "Apply the above formula to differentiate $f(x) = e^x$ at x = 0, $f(x) = e^{âˆ’2x}$ at\n",
    "x = 0, $f(x) = \\cos(x)$ at x = 2$\\pi$ , and $f(x) = \\ln(x)$ at x = 1. \n",
    "\n",
    "Use $h = 0.01$. \n",
    "\n",
    "In each case, write out the error, i.e., the difference between the exact derivative and the result of the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximate derivative of exp(x) at x = 0 is: 1.000017. The error is 0.000017.\n",
      "The approximate derivative of exp(-2*x) at x = 0 is: -2.000133. The error is 0.000133.\n",
      "The approximate derivative of cos(x) at x = 2*pi is: 0.000000. The error is 0.000000.\n",
      "The approximate derivative of ln(x) at x = 0 is: 1.000033. The error is 0.000033.\n"
     ]
    }
   ],
   "source": [
    "# Write a function for numerical differentiation\n",
    "\n",
    "from math import exp, cos, log, pi\n",
    "\n",
    "def diff(f, x, h = 1E-6):\n",
    "   numerator = f(x + h) - f(x - h)\n",
    "   derivative = numerator/(2.0*h)\n",
    "   return derivative\n",
    "   \n",
    "h = 0.01 # The step size\n",
    "\n",
    "x = 0\n",
    "f = exp\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of exp(x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - 1)) \n",
    "# The 'abs' function returns the absolute value.\n",
    "\n",
    "x = 0\n",
    "# Here it is not possible to simply pass in the math module's exp function,\n",
    "# so we need to define our own function instead.\n",
    "def g(x):\n",
    "   return exp(-2*x)\n",
    "f = g\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of exp(-2*x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - (-2.0)))\n",
    "\n",
    "x = 2*pi\n",
    "f = cos\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of cos(x) at x = 2*pi is: %f. The error is %f.\" % (derivative, abs(derivative - 0))\n",
    "\n",
    "x = 1\n",
    "f = log # By default, log(x) is the natural log (i.e. the log to the base 'e')\n",
    "derivative = diff(f, x, h)\n",
    "print \"The approximate derivative of ln(x) at x = 0 is: %f. The error is %f.\" % (derivative, abs(derivative - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.3: Compute the derivative of $\\sin(x)$</span>\n",
    "\n",
    "Compute \n",
    "\n",
    "$$\\frac{d(\\sin x)}{dx}\\qquad\\textrm{at}\\qquad x = 0.8$$\n",
    "\n",
    "using (a) forward differencing and (b) central differencing. \n",
    "\n",
    "Write some code that evaluates these derivatives for decreasing values of $h$ (start with $h=1.0$ and keep halving) and compare the values against the exact solution.\n",
    "\n",
    "Plot the convergence of your two methods.\n",
    "\n",
    "You should get something that looks like this\n",
    "\n",
    "![\"Convergence plot\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/fd_cd_convergence.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating second derivatives\n",
    "\n",
    "Numerical differentiation may be extended to the second derivative by noting that the second derivative is the derivative of the first derivative. That is, if we define a new function $g$ for a second, where:\n",
    "\n",
    "$$ g(x)=f'(x) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ f''(x)=g'(x) $$\n",
    "\n",
    "and so we can just apply our differencing formulae twice in order to achieve a second derivative (and so on for even higher  derivatives).\n",
    "\n",
    "We have noted above that the central difference method, being second-order accurate, is superior to the forward difference method so we will choose to extend that.\n",
    "\n",
    "In order to calculate $f''(x_0)$ using a central difference method, we first calculate $f'(x)$ for each of two half intervals, one to the left of $x_0$ and one to the right:\n",
    "\n",
    "$$ f'\\left(x_0+\\frac{h}{2}\\right)\\approx\\frac{f(x_0+h)-f(x_0)}{h} $$\n",
    "$$  f'\\left(x_0-\\frac{h}{2}\\right)\\approx\\frac{f(x_0)-f(x_0-h)}{h} $$\n",
    "\n",
    "[Of course the things on the RHS are first-order forward and backward differences if we were to consider the LHS at $x_0$.\n",
    "However, by considering the LHS at $x_0\\pm h/2$ they are in this case clearly second-order *central* differences where the denominator of the RHS is $2\\times (h/2)$.]\n",
    "\n",
    "We can now calculate the second derivative using these two values. Note that we know $f'(x)$ at the points $x_0\\pm{h}/{2}$, which are only $h$ rather than $2h$ apart. Hence:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f''(x_0)&\\approx\\frac{f'(x_0+\\frac{h}{2})-f'(x_0-\\frac{h}{2})}{h}\\\\\n",
    "    &\\approx\\frac{\\frac{f(x_0+h)-f(x_0)}{h}-\\frac{f(x_0)-f(x_0-h)}{h}}{h}\\\\\n",
    "    &\\approx\\frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.4: Compute second derivative</span>\n",
    "\n",
    "Calculate the second derivative $f''$ at $x = 1$ using the data below:\n",
    "\n",
    "$f(0.84) = 0.431711$\n",
    "\n",
    "$f(0.92) = 0.398519$\n",
    "\n",
    "$f(1.00) = 0.367879$\n",
    "\n",
    "$f(1.08) = 0.339596$\n",
    "\n",
    "$f(1.16) = 0.313486$\n",
    "\n",
    "You should get 0.0036828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Non-central differencing and differentiation by polynomial fit\n",
    "\n",
    "In this particular case we were given more data than we actually used. An alternative approach would be to use *non-centred differencing*, e.g. the following is also a valid approximation to the second derivative\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f''(x_0)\\approx\\frac{f(x_0+2h)-2f(x_0+h)+f(x_0)}{h^2}\n",
    "\\end{align}$$\n",
    "\n",
    "This can come in handy if we need to approximate the value of derivatives at or near to a boundary where we don't have data beyond that boundary.\n",
    "\n",
    "If we wanted to use all of this data, an alternative would be to fit a polynomial to this data, and then differentiate this analytical expression exactly to approximate the derivative at any point between 0.84 and 1.16 (recalling that extrapolation is dangerous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical methods for ODEs\n",
    "\n",
    "One of the most important applications of numerical mathematics in the sciences is the numerical solution of ordinary differential equations (ODEs). This is a vast topic which rapidly becomes somewhat advanced, so we will restrict ourselves here to a very brief introduction to the solution of first order ODEs. A much more comprehensive treatment of this subject is to be found in the Numerical Methods 2 module.\n",
    "\n",
    "Suppose we have the general first-order ODE:\n",
    "\n",
    "\\begin{align}\n",
    "x'(t)&=f(x(t),t) \\\\\n",
    "x(t_0)&=x_0\n",
    "\\end{align}\n",
    "\n",
    "[Notation: For $x=x(t)$, $\\frac{dx}{dt}=x'=\\dot{x}$.]\n",
    "\n",
    "That is, the derivative of $x$ with respect to $t$ is some known function of $x$ and $t$, and we also know the initial condition of $x$ at some initial time $t_0$.\n",
    "\n",
    "If we manage to solve this equation analytically, the solution will be a function $x(t)$ which is defined for every $t>t_0$. In common with all of the numerical methods we will encounter in this module, our objective is to find an approximate solution to the ODE at a finite set of points. In this case, we will attempt to find approximate solutions at $t=t_0,t_0+h,t_0+2h,t_0+3h,\\ldots$.\n",
    "\n",
    "It is frequently useful to think of the independent variable, $t$, as representing time. A numerical method steps forward in time units of $h$, attempting to calculate $x(t+h)$ in using the previously calculated value $x(t)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler's method\n",
    "\n",
    "To derive a numerical method, we can first turn once again to the Taylor\n",
    "series. In this case, we could write:\n",
    "\n",
    "$$ x(t+h)=x(t)+h x'(t) + O(h^2) $$\n",
    "\n",
    "Using the definition of our ODE above, we can substitute in for $x'(t)$:\n",
    "\n",
    "$$ x(t+h)=x(t)+h f(x(t),t)+ O(h^2)$$\n",
    "\n",
    "Notice that the value of $x$ used in the evaluation of $f$ is that at time $t$. This simple scheme is named **Euler's method** after the 18th century Swiss mathematician, Leonard Euler. \n",
    "\n",
    "This is what is known as an explicit method, because the function $f$ in this relation is evaluated at the old time level $t$\n",
    "-- i.e. we have all the information required at time $t$ to explicitly compute the right-hand-side,\n",
    "and hence easily find the new value for $x(t+h)$. This form of the method is therefore more correctly called either Explicit Euler or Forward Euler.  We could also evaluate the RHS at some time between $t$ and $t+h$ (in the case of $t+h$ this method is called Implicit or Backward Euler) this is more complex to solve for the new $x(t+h)$ but can have advantageous accuracy and stability properties.\n",
    "\n",
    "The formula given is used to calculate the value of $x(t)$ one time step forward from the last known value. The error is therefore the local truncation error. If we actually wish to know the value at some fixed time $T$ then we will have to calculate $(T-t_0)/h$ steps of the method. This sum over $O(1/h)$ steps results in a global truncation error for Euler's method of $O(h)$. In other words, Euler's method is only first-order accurate -- if we halve $h$ we will need to do double the amount of work and the error should correspondingly halve; if we had a second-order method we would expect the error to reduce by a factor of 4 for every doubling in effort!\n",
    "\n",
    "To illustrate Euler's method, and convey the fundamental idea of all time stepping methods, we'll use Euler's method to solve one of the simplest of all ODEs:\n",
    "\n",
    "$$ x'(t)=x(t) $$\n",
    "$$ x(0)=1 $$\n",
    "\n",
    "We know, of course, that the solution to this equation is $x(t)=e^t$, but let's ignore that for one moment and evaluate $x(0.1)$ using Euler's method with steps of $0.05$. The first step is:\n",
    "\n",
    "$$\\begin{align}\n",
    "  x(0.05)&\\approx x(0)+0.05x'(0)\\\\\n",
    "  &\\approx1+.05\\times1\\\\\n",
    "  &\\approx 1.05\n",
    "\\end{align}$$\n",
    "\n",
    "Now that we know $x(0.05)$, we can calculate the second step:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  x(0.1)&\\approx x(0.05)+0.05x(0.05)\\\\\n",
    "  &\\approx 1.05+.05\\times1.05\\\\\n",
    "  &\\approx 1.1025\n",
    "\\end{align}$$\n",
    "\n",
    "Now the actual value of $e^{0.1}$ is around $1.1051$ so we're a couple of percent off even over a very short time interval and only a couple of steps of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.5: Implementing Forward Euler's method</span>\n",
    "\n",
    "Write a function *euler*( *f*, *x0*, *t0*, *t_max*, *h*) that takes as arguments the function $f(x,t)$ on the RHS of our ODE,\n",
    "an initial value for $x$, the start and end time of the integration, and the time step.\n",
    "\n",
    "Use it to integrate the following ODE problems up to time $t=10$\n",
    "\n",
    "$$x'(t)=x(t),\\quad x(0)=1$$\n",
    "\n",
    "and \n",
    "\n",
    "$$x'(t)=\\cos(t),\\quad x(0)=0$$\n",
    "\n",
    "and plot the results.  A template to get you started is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "def euler(f,x0,t0,t_max,h):\n",
    "    x=x0; t=t0\n",
    "    # these lists will store all solution values \n",
    "    # and associated time levels for later plotting\n",
    "    x_all=[x0]; t_all=[t0]\n",
    "    \n",
    "    \n",
    "    while ... add your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return(x_all,t_all)\n",
    "\n",
    "\n",
    "def f(x,t):\n",
    "    val = x\n",
    "    return val\n",
    "\n",
    "(x_all,t_all) = euler(f,1.0,0.0,10.0,0.1)\n",
    "\n",
    "plot(t_all, x_all)\n",
    "xlabel('t');ylabel('x(t)');grid(True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heun's method\n",
    "\n",
    "Euler's method is first-order accurate because it calculates the derivative using only the information available at the beginning of the time step. As we observed previously, higher-order convergence can be obtained if we also employ information from other points in the interval. Heun's method may be derived by attempting to use derivative information at both the start and the end of the interval:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  x(t+h)&\\approx x(t)+\\frac{h}{2}\\left(x'(t)+x'(t+h)\\right)\\\\\n",
    "  &\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(x(t+h),t+h)\\big)\n",
    "\\end{align}$$\n",
    "\n",
    "The difficulty with this approach is that we now require $x(t+h)$ in order to calculate the final term in the equation, and that's what we set out to calculate so we don't know it yet! So at this point we have an example of an implicit algorithm and at this stage the above ODE solver would be referred to as the trapezoidal method if we could solve it exactly for $x(t+h)$.\n",
    "\n",
    "Perhaps the simplest solution to this dilemma, the one adopted in Heun's method, is to use a first guess at $x(t+h)$ calculated using Euler's method:\n",
    "\n",
    "$$ \\tilde{x}(t+h)=x(t)+hf(x,t) $$\n",
    "\n",
    "This first guess is then used to solve for $x(t+h)$ using:\n",
    "\n",
    "$$ x(t+h)\\approx x(t)+\\frac{h}{2}\\big(f(x,t)+f(t+h,\\tilde{x}(t+h))\\big)$$\n",
    "\n",
    "The generic term for schemes of this type is **predictor-corrector**. The initial calculation of $\\tilde{x}(t+h)$ is used to predict the new value of $x$ and then this is used in a more accurate calculation to produce a more correct value. \n",
    "\n",
    "Note that Heun's method is $O(h^2)$, i.e. a second-order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 5.6: Implementing Heun's method</span>\n",
    "\n",
    "Repeat the previous exercise for this method.\n",
    "\n",
    "For some ODEs you know the exact solution to compare the errors between Euler's and Heun's method, and how they vary with time step.\n",
    "\n",
    "You should be able to get a plot that looks like this for the case $x'=x$\n",
    "\n",
    "![\"Comparison between the Euler and Heun method for the solution of a simple ODE.\"](https://raw.githubusercontent.com/ggorman/Numerical-methods-1/master/notebook/images/euler_vs_heun.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Methods II\n",
    "\n",
    "Note that you will do a lot more on the numerical solution of ODEs (and also extend to the solution of PDEs) in the Numerical Methods II course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
