{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott), [Christian Jacobs](http://www.christianjacobs.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture ?: Numerical Linear Algebra III\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* Ill-conditioned matrices\n",
    "* Direct vs iterative/indirect methods\n",
    "* An example iterative algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-conditioned matrices\n",
    "\n",
    "The conditioning (or lack of, i.e. the ill-conditioning) of matrices we are trying to invert (to obtain the inverse, or to find the solution to a linear matrix system) is incredibly important for the success of any algorithm.\n",
    "\n",
    "When we started talking about matrices we noted that as long as the matrix is non-singular, i.e. $\\det(A)\\ne 0$ then an inverse exists, and a linear system with that $A$ has a unique solution.\n",
    "\n",
    "But what happens when we consider a matrix that is nearly singluar, i.e. $\\det(A)$ is very small?\n",
    "\n",
    "Well smallness is a relative term and so we need to ask the question of how large or small $\\det(A)$ is compared to something.\n",
    "\n",
    "That something is the *norm* of the matrix.\n",
    "\n",
    "#### Vector norms\n",
    "\n",
    "Just as for vectors $\\pmb{v}$ (assumed a $n\\times 1$ column vector) where we have multiple possible norms to help us decide quantify the magnitude of a vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\pmb{v}\\|_2 & = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\left(\\sum_{i=1}^n v_i^2 \\right)^{1/2}, &\\quad{\\textrm{the two-norm or Euclidean norm}}\\\\\n",
    "\\|\\pmb{v}\\|_1  & = |v_1| + |v_2| + \\ldots + |v_n| = \\sum_{i=1}^n |v_i|, &\\quad{\\textrm{the one-norm or taxi-cab norm}}\\\\\n",
    "\\|\\pmb{v}\\|_{\\infty}  &= \\max\\{|v_1|,|v_2|, \\ldots, |v_n| = \\max_{i=1}^n |v_i|, &\\quad{\\textrm{the max-norm or infinity norm}}\n",
    "\\end{align}\n",
    "\n",
    "#### Matrix norms\n",
    "\n",
    "We can define measures of the size of matrices, e.g. for $A$ which for complete generality we will assume is of shape $m\\times n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\|A\\|_F & = \\left(\\sum_{i=1}^m \\sum_{j=1}^n A_{ij}^2 \\right)^{1/2}, &\\quad{\\textrm{the matrix two-norm or Euclidean or Frobenius norm}}\\\\\n",
    "\\|A\\|_{\\infty} & = \\max_{i=1}^m \\sum_{j=1}^n|A_{i,j}|, &\\quad{\\textrm{the maximum absolute row-sum norm}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that while the vector and matrix norms give different results, they are consistent or equivalent in that they are always within a constant factor of one another (a result that is true for finite-dimensional or discrete problems as here). This means we don't really need to worry too much about which norm we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n",
      "15.748015748\n",
      "15.748015748\n",
      "15.0\n",
      "17.0\n",
      "13.7930910986\n",
      "13.7930910986\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)\n",
    "print(linalg.norm(A))\n",
    "print(linalg.norm(A,'fro'))        # the Frobenius norm - the default\n",
    "print(linalg.norm(A,numpy.inf))    # the maximum absolute row-sum\n",
    "print(linalg.norm(A,1))            # the maximum absolute column-sum\n",
    "print(linalg.norm(A,2))            # the two-norm - note not the same as the Frobenius norm - also termed the spectral norm\n",
    "print(numpy.sqrt(numpy.real((numpy.max(linalg.eigvals(numpy.dot(A.T,A)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: matrix norms</span>\n",
    "\n",
    "Write some code to explicity compute the two matrix norms defined mathematically above and compare against the values found above using in-built scipy functions.\n",
    "\n",
    "Based on the above code and comments, what is the mathematical definition of the 1-norm and the 2-norm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix conditioning\n",
    "\n",
    "The (ill-)conditioning of a matrix is measured with the matrix condition number:\n",
    "\n",
    "$$\\textrm{cond}(A) = \\|A\\|\\|A^{-1}\\|$$\n",
    "\n",
    "If this is close to one then $A$ is well-conditioned, and it increases with the degree of ill-conditioning, reaching infinity for a singular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n",
      "10.7133718813\n",
      "10.7133718813\n",
      "12.4636165619\n",
      "12.4636165619\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)\n",
    "print(numpy.linalg.cond(A))\n",
    "print(linalg.norm(A,2)*linalg.norm(linalg.inv(A),2))  # so the default condition number uses the matrix two-norm\n",
    "print(numpy.linalg.cond(A,'fro'))\n",
    "print(linalg.norm(A,'fro')*linalg.norm(linalg.inv(A),'fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is expensive to compute, and so in practice the size of the determinant of the matrix is gauges based on the magnitude of the entries of the matrix.\n",
    "\n",
    "We know that a singular matrix does not result in a unique solution to a corresponding linear matrix system. But what are the consequences of near-singularity (ill-conditioning)?\n",
    "\n",
    "Consider the following example\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{cc}\n",
    "    2 & 1 \\\\\n",
    "    2 & 1 + \\epsilon  \\\\\n",
    "  \\end{array}\n",
    "\\right)\\left(\n",
    "  \\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{c}\n",
    "    3 \\\\\n",
    "    0 \\\\\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Clearly when $\\epsilon=0$ the two columns/rows are not linear independent, and hence the determinant of this matrix is zero, the condition number is infinite, and the linear system does not have a solution.\n",
    "\n",
    "Consider a range of small values for $\\epsilon$ and calculate the matrix determinant and condition number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=numpy.array([[ ],[ ]])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find for $\\epsilon=0.001$ that $\\det(A)=0.002$ (i.e. quite a lot smaller than the other coefficients in the matrix) and $\\textrm{cond}(A)\\approx 5000$.\n",
    "\n",
    "Using `numpy.dot(linalg.inv(A),b)` you should also be able to compute the solution $\\pmb{x}=(1501.5,-3000.)^T$.\n",
    "\n",
    "What happens when you make a very small change to the coefficients of the matrix (e.g. set $\\epsilon=0.002$)?\n",
    "\n",
    "You should find that this change of just $0.1\\%$ in one of the coefficients of the matrix results in a $100%$ change in both components of the solution!\n",
    "\n",
    "This is the consequence of the matrix being ill-conditioned - we should not trust the numerical solution to ill-conditioned problems.  A way to see this is to recognise that computers do not perform arithmetic exactly - they necessarily have to truncate numbers at a certain number of significant figures, performing multiple operations with these truncated numbers can lead to an erosion of accuracy. Often this isn't a problem, but these so-called **round-off** errors in algorithms generating $A$, or operating on $A$ as in Gaussian elinination, will lead to small inaccuracies in the coefficients of the matrix. Hence,  will fall foul of the problem seen above where a very small error in an input led to a far larger error in an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round-off errors\n",
    "\n",
    "Note that in this course we are largely going to ignore the limitations of the floating point arithmetic performed by computers, including round-off errors.  \n",
    "\n",
    "This is often the topic of the first lecture of courses, or first chapter of books, on Numerical Methods or Numerical Analysis - do take a look at some examples if you are interested.  \n",
    "\n",
    "Also take a look at *D. Goldberg 1991: What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys 23, Pages 5-48*.\n",
    "\n",
    "For some examples of catastrophic failures due to round off errors see <https://www.ma.utexas.edu/users/arbogast/misc/disasters.html>\n",
    "\n",
    "As an example, consider the mathematical formula\n",
    "\n",
    "$$f(x)=(1-x)^{10}.$$\n",
    "\n",
    "We can of course relatively easily expand this out by hand\n",
    "\n",
    "$$f(x)=1- 10x + 45x^2 - 120x^3 + 210x^4 - 252x^5 + 210x^6 - 120x^7 + 45x^8 - 10x^9 + x^{10}.$$\n",
    "\n",
    "Mathematically these two things are identicial, but numerically different operations will be performed, which should give the same answer. For numbers $x$ away from $1$ these two expresssions do return (pretty much) the same answer.  \n",
    "\n",
    "However, for $x$ close to 1 the second expression adds and subtracts individual terms of increasing size which should largely cancel out, but they don't to sufficient accuracy due to round off errors; these errors accumulate with more and more operations, leading a loss of significant <https://en.wikipedia.org/wiki/Loss_of_significance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010485760000000006 0.00010485760000436464 4.1623815505431594e-11\n",
      "1.0239999999999978e-07 1.0240001356576212e-07 1.3247813024364063e-07\n",
      "9.765625000000086e-14 1.2378986724570495e-13 0.21111273343425307\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return (1. - x)**10\n",
    "\n",
    "def f2(x):\n",
    "    return (1. - 10.*x + 45.*x**2 - 120.*x**3 +\n",
    "           210.*x**4 - 252.*x**5 + 210.*x**6 -\n",
    "           120.*x**7 + 45.*x**8 - 10.*x**9 + x**10)\n",
    "\n",
    "x=0.6\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) # values computed in different ways and their relative difference\n",
    "x=0.8\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) \n",
    "x=0.95\n",
    "print(f1(x),f2(x),1.-f1(x)/f2(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm stability\n",
    "\n",
    "The susceptibility for a numerical algorithm to dampen (inevitable) errors, rather than to magnify them as we have seen in examples above, is terms *stability*.  This is a concern for numerical linear algebra as considered here, as well as for the numerical solution of differential equations as you will see in NM2.  In that case you don't want small errors to grow and accumulate as you propagate the solution to an ODE or PDE forward in time say.\n",
    "\n",
    "If your algorithm is not inherently stable, or has other limitation, you need to understand and appreciate this, as it can cause catastrophic failures! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct vs iterative methods\n",
    "\n",
    "Two types/families of methods exist to solve matrix systems.  These are termed *direct* methods and *iterative* (or *indirect*) methods.\n",
    "\n",
    "Direct methods perform operations on the linear equations (the matrix system), e.g. the substitution of one eqution into another which we performed two weeks ago for your example $2\\times 2$ system considered in MM1. This (and the subsequent Gaussian elimination algorithm) transformed the equations making up the linear system into equivalent ones with the aim of eliminating unknowns from some of the equations and hence allowing for easy solution through back (or forward) substitution.\n",
    "\n",
    "Also, in MM1 you learnt Cramer's rule which gives an explicit formula for the inverse of a matrix, or for the solution of a linear matrix system.  It was pointed out that the computational cost (in terms of arithmetic operations required; also termed complexity) scaled like $(n+1)!$, whereas the Gaussian elimination (which is basically the susbtitution method done above) scaled like $n^3$.  For large $n$ Gaussian elimination will clearly be more efficient - you considered the case where $n=100$ in MM1 for example. $n$ here refers to the number of unknowns or equations, or sometimes termed the *degrees of freedom* of the problem.\n",
    "\n",
    "An advantage of direct methods is that they provide the exact solution (assuming exact arithmetic, i.e. ignoring the round off related issues mentioned above) in a finite number of operations.\n",
    "\n",
    "However, as pointed out previously, $n$ could be billions for hard-core applications such as in weather forecasting. In this case the $n^3$ operations required of a direct algorithm such as Gaussian elimination is also prohibitive. \n",
    "\n",
    "In order to reduce this cost, ideally to a level that is (close to) linear in $n$, *iterative* algorithms were devised. \n",
    "\n",
    "These start with a guess at the solution ($\\pmb{x}_0$), they calculate the residual vector ($A\\pmb{x}_0 - \\pmb{b}$), and its norm (a scalar measure of a vector's size - e.g. the vector *2-norm* is just the square root of the sum of the squares of the components) which will obviously not be zero unless you were very lucky with your initial guess, and then *iteratively* seek to improve on this solution to drive down this residual norm.  This iteration will stop at some small (non-zero) residual norm tolerance level, yielding an approximation to the solution, but not the exact solution we would obtain with direct methods.  The residual norm tolerance stopping criteria therefore needs to be thought about carefully, e.g. depending on how accurate a solution $\\pmb{x}$ we require.\n",
    "\n",
    "We have already considered Gaussian elimination (and back substitution) as examples of direct solution methods. We'll consider an example of an iterative method now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods - Jacobi's method\n",
    "\n",
    "Consider our matrix system\n",
    "\n",
    "$$A\\pmb{x}=\\pmb{b} \\quad \\iff \\quad \\sum_{j=1}^nA_{ij}x_j=b_i,\\quad \\textrm{for}\\quad i=1,2,\\ldots, n.$$\n",
    "\n",
    "Let's rewrite this by pulling out the term involving $x_i$:\n",
    "\n",
    "$$A_{ii}x_i + \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j=b_i,\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "We can then come up with a formula for our unknown $x_i$:\n",
    "\n",
    "$$x_i = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Now of course for each individual $x_i$, all the other components of $\\pmb{x}$ appearing on the RHS are also unknown and so this is an example of an implicit formula which doesn't help us directly, but does suggest the following iterative scheme:\n",
    "\n",
    "* Starting from a guess at the solution $\\pmb{x}^{(0)}$\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Note that for this iteration, for a fixed $k$, it does not matter in which order we perform the operations over $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods - Gauss-Seidel's method\n",
    "\n",
    "We can make a small improvement to Jacobi's method using the updated components of the solution vector as they become available:\n",
    "\n",
    "* Starting from a guess at the solution $\\pmb{x}^{(0)}$\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j< i}}^nA_{ij}x_j^{(k)} - \\sum_{\\substack{j=1\\\\ j> i}}^nA_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Note that as opposed to Jacobi, we can overwrite the entries of $\\pmb{x}$ as they are updated, with Jacobi we need to store both the new as well as the old iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: implementation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: examples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
