{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott), [Christian Jacobs](http://www.christianjacobs.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture ?: Numerical Linear Algebra III\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* Ill-conditioned matrices\n",
    "* Direct vs iterative/indirect methods\n",
    "* An example iternative algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-conditioned matrices\n",
    "\n",
    "The conditioning (or lack of, i.e. the ill-conditioning) of matrices we are trying to invert (to obtain the inverse, or to find the solution to a linear matrix system) is incredibly important for the success of any algorithm.\n",
    "\n",
    "When we started talking about matrices we noted that as long as the matrix is non-singular, i.e. $\\det(A)\\ne 0$ then an inverse exists, and a linear system with that $A$ has a unique solution.\n",
    "\n",
    "But what happens when we consider a matrix that is nearly singluar, i.e. $\\det(A)$ is very small?\n",
    "\n",
    "Well smallness is a relative term and so we need to ask the question of how large or small $\\det(A)$ is compared to something.\n",
    "\n",
    "That something is the *norm* of the matrix.\n",
    "\n",
    "#### Vector norms\n",
    "\n",
    "Just as for vectors $\\pmb{v}$ (assumed a $n\\times 1$ column vector) where we have multiple possible norms to help us decide quantify the magnitude of a vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\pmb{v}\\|_2 & = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2} = \\left(\\sum_{i=1}^n v_i^2 \\right)^{1/2}, &\\quad{\\textrm{the two-norm or Euclidean norm}}\\\\\n",
    "\\|\\pmb{v}\\|_1  & = |v_1| + |v_2| + \\ldots + |v_n| = \\sum_{i=1}^n |v_i|, &\\quad{\\textrm{the one-norm or taxi-cab norm}}\\\\\n",
    "\\|\\pmb{v}\\|_{\\infty}  &= \\max\\{|v_1|,|v_2|, \\ldots, |v_n| = \\max_{i=1}^n |v_i|, &\\quad{\\textrm{the max-norm or infinity norm}}\n",
    "\\end{align}\n",
    "\n",
    "#### Matrix norms\n",
    "\n",
    "We can define measures of the size of matrices, e.g. for $A$ which for complete generality we will assume is of shape $m\\times n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\|A\\|_F & = \\left(\\sum_{i=1}^m \\sum_{j=1}^n A_{ij}^2 \\right)^{1/2}, &\\quad{\\textrm{the matrix two-norm or Euclidean or Frobenius norm}}\\\\\n",
    "\\|A\\|_{\\infty} & = \\max_{i=1}^m \\sum_{j=1}^n|A_{i,j}|, &\\quad{\\textrm{the maximum absolute row-sum norm}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that while the vector and matrix norms give different results, they are consistent or equivalent in that they are always within a constant factor of one another (a result that is true for finite-dimensional or discrete problems as here). This means we don't really need to worry too much about which norm we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n",
      "15.748015748\n",
      "15.748015748\n",
      "15.0\n",
      "17.0\n",
      "13.7930910986\n",
      "13.7930910986\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)\n",
    "print(linalg.norm(A))\n",
    "print(linalg.norm(A,'fro'))        # the Frobenius norm - the default\n",
    "print(linalg.norm(A,numpy.inf))    # the maximum absolute row-sum\n",
    "print(linalg.norm(A,1))            # the maximum absolute column-sum\n",
    "print(linalg.norm(A,2))            # the two-norm - note not the same as the Frobenius norm - also termed the spectral norm\n",
    "print(numpy.sqrt(numpy.real((numpy.max(linalg.eigvals(numpy.dot(A.T,A)))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise: matrix norms</span>\n",
    "\n",
    "Write some code to explicity compute the two matrix norms defined mathematically above and compare against the values found above using in-built scipy functions.\n",
    "\n",
    "Based on the above code and comments, what is the mathematical definition of the 1-norm and the 2-norm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix conditioning\n",
    "\n",
    "The (ill-)conditioning of a matrix is measured with the matrix condition number:\n",
    "\n",
    "$$\\textrm{cond}(A) = \\|A\\|\\|A^{-1}\\|$$\n",
    "\n",
    "If this is close to one then $A$ is well-conditioned, and it increases with the degree of ill-conditioning, reaching infinity for a singular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.   2.   1.]\n",
      " [  6.   5.   4.]\n",
      " [  1.   4.   7.]]\n",
      "10.7133718813\n",
      "10.7133718813\n",
      "12.4636165619\n",
      "12.4636165619\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from scipy import linalg\n",
    "A=numpy.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)\n",
    "print(numpy.linalg.cond(A))\n",
    "print(linalg.norm(A,2)*linalg.norm(linalg.inv(A),2))  # so the default condition number uses the matrix two-norm\n",
    "print(numpy.linalg.cond(A,'fro'))\n",
    "print(linalg.norm(A,'fro')*linalg.norm(linalg.inv(A),'fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is expensive to compute, and so in practice the size of the determinant of the matrix is gauges based on the magnitude of the entries of the matrix.\n",
    "\n",
    "We know that a singular matrix does not result in a unique solution to a corresponding linear matrix system. But what are the consequences of near-singularity (ill-conditioning)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct vs iterative methods\n",
    "\n",
    "Two types/families of methods exist to solve matrix systems.  These are termed *direct* methods and *iterative* (or *indirect*) methods.\n",
    "\n",
    "Direct methods perform operations on the linear equations (the matrix system), e.g. the substitution of one eqution into another which we performed last week for your example $2\\times 2$ system considered in MM1. This (and the subsequent Gaussian elimination algorithm) transformed the equations making up the linear system into equivalent ones with the aim of eliminating unknowns from some of the equations and hence allowing for easy solution through back (or forward) substitution.\n",
    "\n",
    "In MM1 you learnt Cramer's rule which gives an explicit formula for the inverse of a matrix, or for the solution of a linear matrix system.  It was pointed out that the computational cost (in terms of arithmetic operations required; also termed complexity) scaled like $(n+1)!$, whereas the Gaussian elimination (which is basically the susbtitution method done above) scaled like $n^3$.  For large $n$ Gaussian elimination will clearly be more efficient - you considered the case where $n=100$ in MM1 for example. $n$ here refers to the number of unknowns or equations, or sometimes termed the *degrees of freedom* of the problem.\n",
    "\n",
    "However, as pointed out above $n$ could be billions for hard-core applications such as in weather forecasting. In this case the $n^3$ operations required of a direct algorithm such as Gaussian elimination is also prohibitive. \n",
    "\n",
    "In order to reduce this cost, ideally to a level that is (close to) linear in $n$, *iterative* algorithms were devised. \n",
    "\n",
    "These start with a guess at the solution ($\\pmb{x}_0$), they calculate the residual vector ($A\\pmb{x}_0 - \\pmb{b}$), and its norm (a scalar measure of a vector's size - e.g. the vector *2-norm* is just the square root of the sum of the squares of the components) which will obviously not be zero unless you were very lucky with your initial guess, and then *iteratively* seek to improve on this solution to drive down this residual norm.  This iteration will stop at some small (non-zero) residual norm tolerance level, yielding an approximation to the solution, but not the exact solution we would obtain with direct methods.  The residual norm tolerance stopping criteria therefore needs to be thought about carefully, e.g. depending on how accurate a solution $\\pmb{x}$ we require.\n",
    "\n",
    "Last week we considered Gaussian elimination (and back substitution) as examples of direct solution methods. We'll look briefly as iterative methods this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
