{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods 1\n",
    "### [Gerard Gorman](http://www.imperial.ac.uk/people/g.gorman), [Matthew Piggott](http://www.imperial.ac.uk/people/m.d.piggott), [Christian Jacobs](http://www.christianjacobs.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture ?: Numerical Linear Algebra II\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "* More on direct methods: LU factorisation\n",
    "* Direct vs iterative/indirect methods\n",
    "* Ill-conditioned matrices\n",
    "* Example iterative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial pivoting\n",
    "\n",
    "At the end of last week we commented that a problem could occur where the $A_{kk}$ we divide through by in the Gaussian elimination and/or back substitution algorithms might be (near) zero.\n",
    "\n",
    "Using Gaussian elinination as an example, let's again consider the algorithm mid-way working on an arbitrary matrix system, i.e. assume that the first $k$ rows have already been transformed into upper-triangular form, while the equations/rows below are not yet in this form:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrrr|r}\n",
    "    A_{11} & A_{12} & A_{13} & \\cdots & A_{1k}  & \\cdots & A_{1n} & b_1 \\\\\n",
    "    0      & A_{22} & A_{23} & \\cdots & A_{2k} & \\cdots & A_{2n} & b_2 \\\\\n",
    "    0      & 0      & A_{33} & \\cdots & A_{3k}  & \\cdots & A_{3n} & b_3 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\hdashline    \n",
    "    0      & 0      & 0      & \\cdots & A_{kk}  & \\cdots & A_{kn} & b_k \\\\    \n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0      & 0      & 0      & \\cdots & A_{nk}  & \\cdots & A_{nn} & b_n \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note we have drawn the horizontal dashed line one row higher, as we are not going to blindly asssume that it is wise to take the current row $k$ as the pivot row, and $A_{kk}$ as the so-called pivot element.\n",
    "\n",
    "*Partial pivoting* selects the best pivot (row or element) as the one where the $A_{ik}$ (for $i\\ge k$) value is largest (relative to the other values of components in its own row $i$), and then swaps this row with the current $k$ row.\n",
    "\n",
    "To generalise our codes above we would simply need to search for this row, and perform the row swap operation - we'll leave this as a howework exercise for those sufficiently keen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct vs iterative methods\n",
    "\n",
    "Two types/families of methods exist to solve matrix systems.  These are termed *direct* methods and *iterative* (or *indirect*) methods.\n",
    "\n",
    "Direct methods perform operations on the linear equations (the matrix system), e.g. the substitution of one eqution into another which we performed last week for your example $2\\times 2$ system considered in MM1. This (and the subsequent Gaussian elimination algorithm) transformed the equations making up the linear system into equivalent ones with the aim of eliminating unknowns from some of the equations and hence allowing for easy solution through back (or forward) substitution.\n",
    "\n",
    "In MM1 you learnt Cramer's rule which gives an explicit formula for the inverse of a matrix, or for the solution of a linear matrix system.  It was pointed out that the computational cost (in terms of arithmetic operations required; also termed complexity) scaled like $(N+1)!$, whereas the Gaussian elimination (which is basically the susbtitution method done above) scaled like $N^3$.  For large $N$ Gaussian elimination will clearly be more efficient - you considered the case where $N=100$ in MM1 for example.\n",
    "\n",
    "However, as pointed out above $N$ could be billions for hard-core applications such as in weather forecasting. In this case the $N^3$ operations required of a direct algorithm such as Gaussian elimination is also prohibitive. \n",
    "\n",
    "In order to reduce this cost, ideally to a level that is linear in $N$, *iterative* algorithms were devised. These start with a guess at the solution ($\\pmb{x}_0$), they calculate the residual ($A\\pmb{x}_0 - \\pmb{b}$) which will obviously not be zero unless you were very lucky with your initial guess, and then *iteratively* seek to improve on this solution to drive down the residual.  This iteration will stop at some small (non-zero) residual tolerance level, yielding an approximation to the solution, but not the exact solution we would obtain with direct methods.  The residual tolerance stopping criteria therefore needs to be thought about carefully, e.g. depending on how accurate a solution $\\pmb{x}$ we require.\n",
    "\n",
    "Last week we considered Gaussian elimination (and back substitution) as examples of direct solution methods. We'll look briefly as iterative methods this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ill-conditioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
